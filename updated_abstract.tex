\begin{abstract}
LLM-as-a-Judge (LLMaaJ) now underpins scalable evaluation, yet we lack a decisive test of a judge's qualification: can it recover a conversation's latent objective and know when that inference is trustworthy? LLMs degrade under irrelevant or long context; multi-turn jailbreaks further hide goals across turns. We introduce \textbf{ObjexMT}, a benchmark for objective extraction and metacognition. Given a multi-turn transcript, a model must return a one-sentence base objective and a self-reported confidence. Accuracy is computed via LLM-judge semantic similarity to gold objectives, converted to binary correctness by a single human-aligned threshold calibrated once on \textbf{N=300} items ($\tau^\star\!=\!\mathbf{0.66}$). Metacognition is evaluated with ECE, Brier, \emph{Wrong@High-Confidence} (0.80/0.90/0.95), and risk--coverage. Across six models (\texttt{gpt-4.1}, \texttt{claude-sonnet-4}, \texttt{Qwen3-235B-A22B-FP8}, \texttt{kimi-k2}, \texttt{deepseek-v3.1}, \texttt{gemini-2.5-flash}) on \emph{SafeMTData\_Attack600}, \emph{SafeMTData\_1K}, \emph{MHJ}, and \emph{CoSafe}, \texttt{claude-sonnet-4} attains the best objective-extraction accuracy (\textbf{0.493}; 95\% CI [0.479, 0.508]) and the lowest selective risk (AURC \textbf{0.422}); \texttt{kimi-k2} (\textbf{0.483}) and \texttt{deepseek-v3.1} (\textbf{0.482}) form a near-tied second tier. Despite improvements, high-confidence errors remain substantial: Wrong@0.90 ranges from \textbf{39.2\%} (\texttt{claude-sonnet-4}) to \textbf{55.0\%} (\texttt{Qwen3-235B-A22B-FP8}). ObjexMT thus supplies an actionable test for LLM judges: when objectives are not explicit, judges often misinfer them; we recommend exposing objectives when feasible and gating decisions by confidence otherwise. Code and data at \repolink.
\end{abstract}