{
  "kimi-k2": {
    "model": "kimi-k2",
    "overall_accuracy": 0.49134455774247093,
    "accuracy_95ci_lower": 0.4759248280768319,
    "accuracy_95ci_upper": 0.5062900165994784,
    "coverage": 1.0,
    "n_scored": 4217,
    "n_total": 4217,
    "mean_similarity_score": 0.5803983874792507,
    "mean_self_confidence": 0.8658774009959687,
    "extraction_error_rate_pct": 0.0,
    "per_source": {
      "SafeMTData_Attack600": {
        "accuracy": 0.3466666666666667,
        "n_scored": 600
      },
      "SafeMTData_1K": {
        "accuracy": 0.6410714285714286,
        "n_scored": 1680
      },
      "MHJ_local": {
        "accuracy": 0.8733705772811918,
        "n_scored": 537
      },
      "CoSafe": {
        "accuracy": 0.22714285714285715,
        "n_scored": 1400
      }
    },
    "spread": 0.6462277201383346,
    "ece": 0.37453284325349767,
    "brier_score": 0.38156410362817167,
    "wrong_at_080": 0.48212351029252437,
    "wrong_at_090": 0.43888647237929534,
    "wrong_at_095": 0.3811357074109721,
    "aurc": 0.4241743822768729,
    "categorical_accuracy": 0.4496087265828788,
    "per_source_robustness": {
      "SafeMTData_Attack600": 0.2866666666666667,
      "SafeMTData_1K": 0.5916666666666667,
      "MHJ_local": 0.8305400372439479,
      "CoSafe": 0.20285714285714285
    }
  },
  "deepseek-v3.1": {
    "model": "deepseek-v3.1",
    "overall_accuracy": 0.4925302347640503,
    "accuracy_95ci_lower": 0.47853924590941427,
    "accuracy_95ci_upper": 0.5074816220061655,
    "coverage": 1.0,
    "n_scored": 4217,
    "n_total": 4217,
    "mean_similarity_score": 0.5808750296419256,
    "mean_self_confidence": 0.8712876452454352,
    "extraction_error_rate_pct": 0.0,
    "per_source": {
      "SafeMTData_Attack600": {
        "accuracy": 0.33666666666666667,
        "n_scored": 600
      },
      "SafeMTData_1K": {
        "accuracy": 0.6375,
        "n_scored": 1680
      },
      "MHJ_local": {
        "accuracy": 0.8342644320297952,
        "n_scored": 537
      },
      "CoSafe": {
        "accuracy": 0.2542857142857143,
        "n_scored": 1400
      }
    },
    "spread": 0.5799787177440809,
    "ece": 0.37918425420915347,
    "brier_score": 0.3835340289305193,
    "wrong_at_080": 0.48842050481394744,
    "wrong_at_090": 0.4414668547249647,
    "wrong_at_095": 0.35564053537284895,
    "aurc": 0.4349570247464495,
    "categorical_accuracy": 0.4491344557742471,
    "per_source_robustness": {
      "SafeMTData_Attack600": 0.27166666666666667,
      "SafeMTData_1K": 0.5910714285714286,
      "MHJ_local": 0.8081936685288641,
      "CoSafe": 0.21714285714285714
    }
  },
  "gemini-2.5-flash": {
    "model": "gemini-2.5-flash",
    "overall_accuracy": 0.4531657576476168,
    "accuracy_95ci_lower": 0.43703462176903013,
    "accuracy_95ci_upper": 0.4678681527152004,
    "coverage": 1.0,
    "n_scored": 4217,
    "n_total": 4217,
    "mean_similarity_score": 0.562869338392222,
    "mean_self_confidence": 0.8936589992885939,
    "extraction_error_rate_pct": 0.0,
    "per_source": {
      "SafeMTData_Attack600": {
        "accuracy": 0.22833333333333333,
        "n_scored": 600
      },
      "SafeMTData_1K": {
        "accuracy": 0.5815476190476191,
        "n_scored": 1680
      },
      "MHJ_local": {
        "accuracy": 0.8286778398510242,
        "n_scored": 537
      },
      "CoSafe": {
        "accuracy": 0.25142857142857145,
        "n_scored": 1400
      }
    },
    "spread": 0.6003445065176909,
    "ece": 0.4509034858904436,
    "brier_score": 0.44827668958975575,
    "wrong_at_080": 0.5273865414710485,
    "wrong_at_090": 0.5206323687031083,
    "wrong_at_095": 0.45413961038961037,
    "aurc": 0.4415967581682104,
    "categorical_accuracy": 0.4152240929570785,
    "per_source_robustness": {
      "SafeMTData_Attack600": 0.195,
      "SafeMTData_1K": 0.5333333333333333,
      "MHJ_local": 0.7914338919925512,
      "CoSafe": 0.22357142857142856
    }
  }
}